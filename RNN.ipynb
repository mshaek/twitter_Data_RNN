{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BozShcXCA_3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import all libraries\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "nlp= spacy.load(\"en\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k3n7oNyCGXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the dataset\n",
        "train = pd.read_csv(\"/content/training.1600000.processed.noemoticon.csv\", \n",
        "                    encoding= \"latin-1\")\n",
        "y_train = train[train.columns[0]]\n",
        "x_train = train[train.columns[5]]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pq2dsyyC3AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into test and train \n",
        "from sklearn.model_selection import train_test_split\n",
        "trainset1x, trainset2x, trainset1y, trainset2y = train_test_split(x_train.values, \n",
        "                                                                  y_train.values, \n",
        "                                                                  test_size = 0.02, \n",
        "                                                                  random_state= 42)\n",
        "trainset2y= pd.get_dummies(trainset2y)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N6ny_63C28v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to remove stopwords\n",
        "def stopwords(sentence):\n",
        "  new = []\n",
        "  sentence= nlp(sentence)\n",
        "  for w in sentence:\n",
        "    if (w.is_stop == False) & (w.pos_ !=\"PUNCT\"):\n",
        "      new.append(w.string.strip())\n",
        "    c=\" \".join(str(x) for x in new)\n",
        "  return c"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfAvHoynC24_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# funtions to lematize the tweets\n",
        "def lemmatize(sentence):\n",
        "  sentence= nlp(sentence)\n",
        "  str=\" \"\n",
        "  for w in sentence:\n",
        "    str+=\" \"+w.lemma_\n",
        "  return nlp(str)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwVePQniC21K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the glove model\n",
        "def loadGloveModel(gloveFile):\n",
        "  print(\"Loading Glove Model\")\n",
        "  f= open(gloveFile, 'r')\n",
        "  model = {}\n",
        "  for line in f:\n",
        "    splitLine= line.split()\n",
        "    word = splitLine[0]\n",
        "    embedding = [float(val) for val in splitLine[1:]]\n",
        "    model[word] = embedding\n",
        "  print (\"Done.\"), len(model), (\" words loaded!\")\n",
        "  return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUncbaMUC2vE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6ec3aa70-566e-4440-8725-20834f73f1b2"
      },
      "source": [
        "# save the glove model\n",
        "model = loadGloveModel(\"/content/glove.twitter.27B.200d.txt\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztM6dIe2C2kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorising the sentences\n",
        "def sent_vectorizer(sent, model):\n",
        "  sent_vec = np.zeros(200)\n",
        "  numw = 0\n",
        "  for w in sent.split():\n",
        "    try: \n",
        "      sent_vec = np.add(sent_vec, model[str(w)])\n",
        "      numw+=1\n",
        "    except:\n",
        "      pass\n",
        "  return sent_vec"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXca56k0C2Py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#obtain a clean vector\n",
        "cleanvector=[]\n",
        "for i in range(trainset2x.shape[0]):\n",
        "  document = trainset2x[i]\n",
        "  document = document.lower()\n",
        "  document = lemmatize(document)\n",
        "  document = str(document)\n",
        "  cleanvector.append(sent_vectorizer(document, model))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUqD3fuZHPf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the input and output in proper shape\n",
        "cleanvector = np.array(cleanvector)\n",
        "cleanvector = cleanvector.reshape(len(cleanvector), 200,1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqlr7nFbHPP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0185aafc-8ad4-4752-c72e-a27e07f597b0"
      },
      "source": [
        "#tokenizing the sequences\n",
        "tokenizer = Tokenizer(num_words=16000)\n",
        "tokenizer.fit_on_texts(trainset2x)\n",
        "sequences= tokenizer.texts_to_sequences(trainset2x)\n",
        "word_index= tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=15, padding=\"post\")\n",
        "print(data.shape) "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1873 unique tokens.\n",
            "(400, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyYVGwCIHPDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape the data and preparing to train\n",
        "data = data.reshape(len(cleanvector),15,1)\n",
        "#from sklearn.model_selection import train_test_split\n",
        "trainx, validx, trainy, validy= train_test_split(data, trainset2y, \n",
        "                                                 test_size=0.3, random_state=42)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXfrRQNaPIX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the number of words\n",
        "nb_words= len(tokenizer.word_index)+1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyJt28zsPIMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91642969-996f-4f44-836c-6b1124dc8c4a"
      },
      "source": [
        "#obtain the embedding matrix\n",
        "embedding_matrix = np.zeros((nb_words, 200))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = model.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i]= embedding_vector\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis =1) == 0))\n",
        "\n",
        "trainy= np.array(trainy)\n",
        "validy= np.array(validy)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUuGL8zhPIAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building a simple RNN model\n",
        "def modelbuild():\n",
        "  model = Sequential()\n",
        "  model.add(keras.layers.InputLayer(input_shape=(15,1)))\n",
        "  keras.layers.embeddings.Embedding(nb_words, 15, weights=[embedding_matrix], \n",
        "                                    input_length=15, trainable=False)\n",
        "  \n",
        "  model.add(keras.layers.recurrent.SimpleRNN(units=100, activation='relu', \n",
        "                                             use_bias = True))\n",
        "  model.add(keras.layers.Dense(units=1000, input_dim=2000, activation='sigmoid'))\n",
        "  model.add(keras.layers.Dense(units=500, input_dim=1000, activation='relu'))\n",
        "  model.add(keras.layers.Dense(units=2, input_dim=500, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPcPlru7PHzM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "83dd5556-b309-42f7-d45a-be2bac69916d"
      },
      "source": [
        "# compiling the model\n",
        "finalmodel= modelbuild()\n",
        "finalmodel.fit(trainx, trainy, epochs= 10, batch_size=120, \n",
        "               validation_data=(validx,validy))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 280 samples, validate on 120 samples\n",
            "Epoch 1/10\n",
            "280/280 [==============================] - 1s 2ms/step - loss: 2.9190 - accuracy: 0.5036 - val_loss: 0.8513 - val_accuracy: 0.5083\n",
            "Epoch 2/10\n",
            "280/280 [==============================] - 0s 287us/step - loss: 1.4458 - accuracy: 0.4821 - val_loss: 1.4819 - val_accuracy: 0.4917\n",
            "Epoch 3/10\n",
            "280/280 [==============================] - 0s 263us/step - loss: 1.0509 - accuracy: 0.4929 - val_loss: 1.2177 - val_accuracy: 0.5083\n",
            "Epoch 4/10\n",
            "280/280 [==============================] - 0s 293us/step - loss: 1.1289 - accuracy: 0.5321 - val_loss: 0.9822 - val_accuracy: 0.5083\n",
            "Epoch 5/10\n",
            "280/280 [==============================] - 0s 268us/step - loss: 0.7671 - accuracy: 0.5393 - val_loss: 0.7993 - val_accuracy: 0.4917\n",
            "Epoch 6/10\n",
            "280/280 [==============================] - 0s 265us/step - loss: 0.7516 - accuracy: 0.4750 - val_loss: 0.7995 - val_accuracy: 0.4917\n",
            "Epoch 7/10\n",
            "280/280 [==============================] - 0s 266us/step - loss: 0.6635 - accuracy: 0.5786 - val_loss: 0.7479 - val_accuracy: 0.5250\n",
            "Epoch 8/10\n",
            "280/280 [==============================] - 0s 278us/step - loss: 0.6417 - accuracy: 0.6000 - val_loss: 0.7845 - val_accuracy: 0.4833\n",
            "Epoch 9/10\n",
            "280/280 [==============================] - 0s 267us/step - loss: 0.6066 - accuracy: 0.6643 - val_loss: 0.7460 - val_accuracy: 0.4667\n",
            "Epoch 10/10\n",
            "280/280 [==============================] - 0s 253us/step - loss: 0.6003 - accuracy: 0.7107 - val_loss: 0.7836 - val_accuracy: 0.5083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1100474438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXUqTnSHOvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}